{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQl6BxbTv-eK"
      },
      "source": [
        "# Activations\n",
        "\n",
        "В этой тетрадке мы напишем собственную реализацию функций активации\n",
        "\n",
        "\n",
        "Запрещено внутри своей реализации создавать класс активации из pytorch и просто применять его. Разрешено исползовать простые ф-ии pytorch типа [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html) и т д\n",
        "\n",
        "Если у ф-ии активации есть дополнительные аргументы, значение по умолчанию должно быть такое же как в реализации PyTorch\n",
        "\n",
        "**Материалы по pytorch:**\n",
        "\n",
        "* [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNMlqoCiv-eM"
      },
      "source": [
        "## Prerequirements\n",
        "\n",
        "```\n",
        "pip install torch numpy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uR_-L-78De7T"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/a/60658965/7286121\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def write_and_run(line, cell):\n",
        "    argz = line.split()\n",
        "    file = argz[-1]\n",
        "    mode = 'w'\n",
        "    if len(argz) == 2 and argz[0] == '-a':\n",
        "        mode = 'a'\n",
        "    with open(file, mode) as f:\n",
        "        f.write(cell)\n",
        "    get_ipython().run_cell(cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-24T12:02:03.697132Z",
          "iopub.status.busy": "2021-01-24T12:02:03.696647Z",
          "iopub.status.idle": "2021-01-24T12:02:05.150001Z",
          "shell.execute_reply": "2021-01-24T12:02:05.148949Z",
          "shell.execute_reply.started": "2021-01-24T12:02:03.697089Z"
        },
        "id": "ovtEtMOxv-eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qGTRWWFo5pOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%write_and_run myrelu.py \n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyReLU(nn.Module):\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # hint! Входной тензор нужно скопировать,\n",
        "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
        "        input_clone = torch.clone(input)\n",
        "        a = torch.zeros(len(input_clone))\n",
        "        return torch.maximum(a, input_clone)\n",
        "\n"
      ],
      "metadata": {
        "id": "9rZXbfck6WPO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cRV-Ea2N5zDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%write_and_run myleakyrelu.py \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyLeakyReLU(nn.Module):\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # hint! Входной тензор нужно скопировать,\n",
        "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
        "        input_clone = torch.clone(input)\n",
        "        a = torch.clone(input)\n",
        "        input_clone[input_clone < 0] = a[a < 0] * 0.01\n",
        "        return input_clone\n",
        "\n"
      ],
      "metadata": {
        "id": "1ouIrrlq63uN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
        "\n"
      ],
      "metadata": {
        "id": "YS2RXTz_55iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%write_and_run mysigmoid.py \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MySigmoid(nn.Module):\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # для этого класса копировать входной тензор уже нет необходимости, почему?\n",
        "        return 1 / (1 + torch.exp(-input))\n"
      ],
      "metadata": {
        "id": "QTHnuZQA67ga"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZs035Rv-eS"
      },
      "source": [
        "## Задание 4\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CT1IEmzFv-eT"
      },
      "outputs": [],
      "source": [
        "%%write_and_run myelu.py \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MyELU(nn.Module):        \n",
        "    def forward(self, input):\n",
        "        # для этого класса копировать входной тензор уже нет необходимости, почему?\n",
        "        alpha = 1\n",
        "        input_clone = torch.clone(input)\n",
        "        a = torch.clone(input_clone)\n",
        "        input_clone[input_clone < 0] = alpha * (torch.exp(a[a < 0]) - 1)\n",
        "        return input_clone\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUfu8arDKoNm"
      },
      "source": [
        "## Тест\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uAagGpwSKqBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de64d067-702f-4622-dc72-4eef02d8ed92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyReLU()\n",
            "MyLeakyReLU()\n",
            "MySigmoid()\n",
            "MyELU()\n"
          ]
        }
      ],
      "source": [
        "import pytest\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _check_pytorch_module_was_not_used(file, module):\n",
        "\n",
        "    file = open(file, mode='r')\n",
        "    \n",
        "    assert module not in file.read(), \"pytorch module must not be used in you activation implementation\"\n",
        "    \n",
        "    file.close()\n",
        "\n",
        "    return\n",
        "\n",
        "def _test_activation(myactivation, torch_activation):\n",
        "    print(myactivation)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        randinput = torch.rand([100])\n",
        "        myactivation_output = myactivation(randinput)\n",
        "\n",
        "        assert id(myactivation_output) != id(randinput), 'pytorch activation function must return new tensor'\n",
        "\n",
        "        for _ in range(100):\n",
        "            randinput = torch.rand([5, 5, 5])\n",
        "\n",
        "            assert torch.allclose(myactivation(randinput), torch_activation(randinput)), 'activation output is not equals to touch ones output'\n",
        "\n",
        "def test_relu():\n",
        "    from myrelu import MyReLU\n",
        "\n",
        "    my_activation = MyReLU()\n",
        "    \n",
        "    _check_pytorch_module_was_not_used(\"myrelu.py\", '.ReLU(')\n",
        "    _test_activation(my_activation, nn.ReLU())\n",
        "\n",
        "def test_leaky_relu():\n",
        "    from myleakyrelu import MyLeakyReLU\n",
        "\n",
        "    my_activation = MyLeakyReLU()\n",
        "    \n",
        "    _check_pytorch_module_was_not_used(\"myleakyrelu.py\", '.LeakyReLU(')\n",
        "    _test_activation(my_activation, nn.LeakyReLU())\n",
        "\n",
        "def test_sigmoid():\n",
        "    from mysigmoid import MySigmoid\n",
        "\n",
        "    my_activation = MySigmoid()\n",
        "    \n",
        "    _check_pytorch_module_was_not_used(\"mysigmoid.py\", '.MySigmoid(')\n",
        "    _test_activation(my_activation, nn.Sigmoid())\n",
        "\n",
        "def test_elu():\n",
        "    from myelu import MyELU\n",
        "\n",
        "    my_activation = MyELU()\n",
        "    \n",
        "    _check_pytorch_module_was_not_used(\"myelu.py\", '.MyELU(')\n",
        "    _test_activation(my_activation, nn.ELU())\n",
        "\n",
        "\n",
        "test_relu()\n",
        "test_leaky_relu()\n",
        "test_sigmoid()\n",
        "test_elu()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bNMlqoCiv-eM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "toc-autonumbering": false,
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}